# NVIDIA：2024年，他们再次打破了自己创下的纪录

三个月前，英伟达在GTC大会上扔出了一颗重磅炸弹——Blackwell架构。

这不是一次简单的产品迭代。用黄仁勋的话说，这是"世界上最强大的AI芯片"。

让我们看看这些数字：B200 Blackwell GPU，单卡训练性能达到20 PFLOPS，推理性能达到40 PFLOPS。比上一代H100快了整整7倍。

这是怎么做到的？

## Blackwell：把两颗芯片变成一个

Blackwell采用了一个大胆的设计——两个芯片互连，封装在一起，但在逻辑上它们是一颗芯片。

这就像把两个超级跑车引擎拼在一起，用一个大脑控制它们。通过业界首个10 TB/s的芯片间互联，两颗芯片可以共享显存、协同计算。

结果就是：2080亿个晶体管，192GB HBM3e显存，8TB/s的带宽。

这些数字意味着什么？

意味着训练像GPT-4这样的大模型，成本和能耗都可以大幅降低。英伟达给出的数据是：相比Hopper架构，Blackwell能将训练成本降低25倍，能耗降低25倍。

这已经不是进步了，这是跨越。

## GB200：超级计算机的超级芯片

真正的狠招，是GB200 Grace Blackwell超级芯片。

把两颗B200 GPU和一颗Grace CPU放在一起，组成一个计算单元。然后，用NVLink技术把72个这样的单元连接起来，就变成了GB200 NVL72。

这是一台完整的AI超级computer。一个机架，就能承载1.4 Exaflops的算力。

这是什么概念？

当年用10000颗H100才能跑的训练任务，现在可能只需要几十颗GB200就搞定了。这让AI模型的训练门槛，被一降再降。

这也是为什么各大科技公司都在疯狂囤积英伟达的芯片。谁先拿到这些算力，谁就能在AI竞赛中占据先机。

## H200：过渡期的王者

在Blackwell大规模量产之前，H200正在承上启下。

H200的核心升级是显存——从H100的80GB HBM3升级到141GB HBM3e，带宽也从3.35TB/s提升到4.8TB/s。

显存容量对AI训练意味着什么？意味着更大的模型、更长的上下文、更高效的推理。

Meta、Google、微软都在抢H200。因为这是一个现实的选择——Blackwell还得等，但AI竞赛等不起。

## 消费级市场的等待

RTX 50系列在哪里？

这是所有游戏玩家都在问的问题。目前的消息是，2025年某个时候，我们会看到RTX 5090和RTX 5080。

Blackwell架构会不会下放到消费级市场？这还是一个悬念。但按照英伟达的产品节奏，大概率会的。

毕竟，游戏玩家是英伟达的基本盘。不能因为AI市场赚了大钱，就把老用户忘了吧？

## AI研究的全面进攻

硬件的突破之外，英伟达的AI研究也在全面铺开。

NIM（NVIDIA Inference Microservices）让AI模型的部署变得像搭积木一样简单。开发者不需要懂底层架构，就能调用各种AI能力。

Omniverse平台正在重新定义"数字孪生"。工厂、城市、甚至整个地球，都能在虚拟世界中被精确模拟和优化。

自动驾驶领域，Thor芯片已经准备好了——算力达到2000 TOPS。这不仅仅是智能汽车，这是装轮子的超级计算机。

## 结局的开始？

2024年，英伟达的市值冲破了3万亿美元。

这意味着英伟达成为了和苹果、微软并驾齐驱的科技巨头。

但真正的挑战，才刚刚开始。

竞争对手不会就此认输。AMD的MI300、Intel的Gaudi，都在努力追赶。各国政府的监管压力也在增大。

更重要的是，AI的浪潮能持续多久？没有人知道确切的答案。

但有一点可以肯定：

只要AI还在进化，算力就会越来越重要。而算力，就是英伟达的语言。

下一个十年的科技革命，大概率还是从一颗叫做GPU的芯片开始。

至于未来是什么样子，我们拭目以待。
